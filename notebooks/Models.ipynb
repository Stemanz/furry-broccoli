{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C2xSCon8eCJC"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "gOo4qQWw5N_w"
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "\n",
    "    \"\"\"Creates a Dataset object, that processes source images and cointains\n",
    "       output training/validation tiles.\n",
    "        \n",
    "        params\n",
    "        ======\n",
    "        \n",
    "        folder name: <str> the folder where to look for the images\n",
    "        tile_size: <int>: images will be sliced into tile_size × tile_size squares\n",
    "        \n",
    "        clean_tag: <str>: an identifier for the clean, non-noisy images\n",
    "        noise_tag: <str>: an identifier for the noisy images\n",
    "        \n",
    "        Note: all images should come in pair, clean and noisy, with the\n",
    "        appropriate tags. See for reference:\n",
    "        https://github.com/Stemanz/furry-broccoli/raw/main/datasets/standard_dataset/\n",
    "        \n",
    "        funcs\n",
    "        =====\n",
    "        \n",
    "        self.make_dataset(): processes the images contained in the <folder_name>\n",
    "             to squares of <tile_size> pixels.\n",
    "             \n",
    "             produces\n",
    "             ========\n",
    "             self.clean_tiles_: <np.array> with tiles from clean images \n",
    "             self.noise_tiles_: <np.array> with tiles from noisy images\n",
    "             \n",
    "             When an image is split into tiles, each tile is split into\n",
    "             R, G, B channels and added to the growing list of tiles.\n",
    "             \n",
    "             Each tile is converted to a <np.array>, values untouched (0-255)      \n",
    "             \n",
    "\n",
    "        self.make_rgb_dataset(): processes the images contained in the <folder_name>\n",
    "             to squares of <tile_size> pixels.\n",
    "             \n",
    "             produces\n",
    "             ========\n",
    "             self.clean_tiles_r_: <list> with tiles from the red channel of clean images\n",
    "             self.clean_tiles_g_: <list> with tiles from the green channel of clean images\n",
    "             self.clean_tiles_b_: <list> with tiles from the blue channel of clean images\n",
    "             self.noise_tiles_r_: <list> with tiles from the red channel of noise images\n",
    "             self.noise_tiles_g_: <list> with tiles from the green channel of noise images\n",
    "             self.noise_tiles_b_: <list> with tiles from the blue channel of noise images\n",
    "             \n",
    "             Each tile is converted to a <np.array>, values untouched (0-255)\n",
    "        \n",
    "        \n",
    "        self.shuffle_dataset(): shuffle all tiles (tile correspondences are maintained)\n",
    "        \n",
    "        self.shuffle_rgb_dataset(): shuffle all tiles (tile correspondences are maintained)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, folder_name, tile_size=28, \n",
    "                 clean_tag=\"ISO200\", noise_tag=\"ISO1600\",\n",
    "                 img_type=\"JPG\"\n",
    "                ):\n",
    "        self.folder_name = folder_name\n",
    "        self.tile_size = tile_size\n",
    "        \n",
    "        self.dataset_shuffled = False\n",
    "        self.rgb_dataset_shuffled = False\n",
    "        \n",
    "        # loading image names from dataset directory ===\n",
    "        # for Python < 3.10 with limited glob functionality\n",
    "        self.basedir = Path(os.getcwd())\n",
    "        os.chdir(folder_name)\n",
    "        img_files = glob(f\"*.{img_type}\")\n",
    "        img_files = [x for x in img_files if x.endswith(f\".{img_type}\")]\n",
    "        os.chdir(self.basedir)\n",
    "        \n",
    "        self.clean_pics_filenames = sorted([x for x in img_files if clean_tag in x])\n",
    "        self.noise_pics_filenames = sorted([x for x in img_files if noise_tag in x])\n",
    "        \n",
    "        try:\n",
    "            assert len(self.clean_pics_filenames) == len(self.noise_pics_filenames)\n",
    "        except:\n",
    "            print(\"**error**: mismatched length of clean and noise images lists. Details (clean/noise):\")\n",
    "            print(len(self.clean_pics_filenames))\n",
    "            print(len(self.noise_pics_filenames))\n",
    "            print(self.clean_pics_filenames)\n",
    "            print(self.noise_pics_filenames)\n",
    "        \n",
    "        if len(self.clean_pics_filenames) == 0:\n",
    "            raise TypeError(f\"Are your sure the specified folders contains any suitable {img_type} image?\")\n",
    "\n",
    "    \n",
    "    def _load_pic(self, image_name, folder_name):\n",
    "\n",
    "        \"\"\"\n",
    "        Assumes a subdirectory <folder name> containing the\n",
    "        image <image_name> to load.\n",
    "\n",
    "        params\n",
    "        ======\n",
    "\n",
    "        image_name: <str>\n",
    "        folder_name: <str>\n",
    "        \"\"\"\n",
    "\n",
    "        fullpath = Path(folder_name, image_name)\n",
    "        picture = Image.open(fullpath)\n",
    "        return picture\n",
    "    \n",
    "\n",
    "    def _crop_in_tiles(self, image, tile_size=28, shift=0):\n",
    "\n",
    "        \"\"\"\n",
    "        This function crops an image in several tiles\n",
    "        tile_size × tile_size squares, yielding a tile\n",
    "        every iteration.\n",
    "\n",
    "        If the input image is not a perfect multiple of\n",
    "        a(tile_size) × b(tile_size), non-square tiles are NOT\n",
    "        YIELDED.\n",
    "\n",
    "        params\n",
    "        ======\n",
    "\n",
    "        image: a Pillow open image\n",
    "        tile_size: <int> pixels; size of the tile side\n",
    "        shift: <int>: the offset from 0,0 in pixels\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(tile_size, int)\n",
    "        assert isinstance(shift, int)\n",
    "\n",
    "        width, height = image.size\n",
    "\n",
    "        #calculate coordinates of every tile\n",
    "        for x in range (0+shift, width, tile_size):\n",
    "            if width - x < tile_size:\n",
    "                continue\n",
    "\n",
    "            for y in range (0+shift, height, tile_size):\n",
    "                if height - y < tile_size:\n",
    "                    continue\n",
    "\n",
    "                # tile coord ===\n",
    "                tile_coord = (\n",
    "                    x, y, # upper left coords\n",
    "                    x + tile_size, y + tile_size # lower right coords\n",
    "                )\n",
    "\n",
    "                tile = image.crop(tile_coord)\n",
    "                yield tile\n",
    "        \n",
    "\n",
    "    def _split_into_channels(self, image, as_array=False):\n",
    "        \n",
    "        if not as_array:\n",
    "            return [image.getchannel(x) for x in \"RGB\"]\n",
    "        else:\n",
    "            return [np.array(image.getchannel(x)) for x in \"RGB\"]\n",
    "    \n",
    "\n",
    "    def make_rgb_dataset(self):\n",
    "        clean_pics = (self._load_pic(x, self.folder_name) for x in self.clean_pics_filenames)\n",
    "        noise_pics = (self._load_pic(x, self.folder_name) for x in self.noise_pics_filenames)\n",
    "\n",
    "        self.clean_tiles_r_ = []\n",
    "        self.clean_tiles_g_ = []\n",
    "        self.clean_tiles_b_ = []\n",
    "        self.noise_tiles_r_ = []\n",
    "        self.noise_tiles_g_ = []\n",
    "        self.noise_tiles_b_ = []\n",
    "        \n",
    "        for clean in clean_pics:\n",
    "            tiles = self._crop_in_tiles(clean, tile_size=self.tile_size,)\n",
    "            for tile in tiles:\n",
    "                r,g,b = self._split_into_channels(tile, as_array=True)\n",
    "                self.clean_tiles_r_.append(r)\n",
    "                self.clean_tiles_g_.append(g)\n",
    "                self.clean_tiles_b_.append(b)\n",
    "        \n",
    "        for noise in noise_pics:\n",
    "            tiles = self._crop_in_tiles(noise, tile_size=self.tile_size,)\n",
    "            for tile in tiles:\n",
    "                r,g,b = self._split_into_channels(tile, as_array=True)\n",
    "                self.noise_tiles_r_.append(r)\n",
    "                self.noise_tiles_g_.append(g)\n",
    "                self.noise_tiles_b_.append(b)\n",
    "\n",
    "        # final transform of each list into a np.array\n",
    "        \n",
    "        self.clean_tiles_r_ = np.array(self.clean_tiles_r_)\n",
    "        self.clean_tiles_g_ = np.array(self.clean_tiles_g_)\n",
    "        self.clean_tiles_b_ = np.array(self.clean_tiles_b_)\n",
    "        self.noise_tiles_r_ = np.array(self.noise_tiles_r_)\n",
    "        self.noise_tiles_g_ = np.array(self.noise_tiles_g_)\n",
    "        self.noise_tiles_b_ = np.array(self.noise_tiles_b_)\n",
    "        \n",
    "        \n",
    "    def make_dataset(self):\n",
    "        clean_pics = (self._load_pic(x, self.folder_name) for x in self.clean_pics_filenames)\n",
    "        noise_pics = (self._load_pic(x, self.folder_name) for x in self.noise_pics_filenames)\n",
    "\n",
    "        # these will store tile1_R, tile1_G, tile1_B, tile2_R, tile2_G, ..\n",
    "        self.clean_tiles_ = []\n",
    "        self.noise_tiles_ = []\n",
    "\n",
    "        for clean in clean_pics:\n",
    "            tiles = self._crop_in_tiles(clean, tile_size=self.tile_size,)\n",
    "            for tile in tiles:\n",
    "                self.clean_tiles_.extend(self._split_into_channels(tile, as_array=True))\n",
    "        \n",
    "        for noise in noise_pics:\n",
    "            tiles = self._crop_in_tiles(noise, tile_size=self.tile_size,)\n",
    "            for tile in tiles:\n",
    "                self.noise_tiles_.extend(self._split_into_channels(tile, as_array=True))\n",
    "\n",
    "        # final transform of each list into a np.array\n",
    "        self.clean_tiles_ = np.array(self.clean_tiles_)\n",
    "        self.noise_tiles_ = np.array(self.noise_tiles_)\n",
    "\n",
    "\n",
    "    def shuffle_dataset(self):\n",
    "        if hasattr(self, \"clean_tiles_\"):\n",
    "            shuffler = np.random.permutation(len(self.clean_tiles_))\n",
    "            self.clean_tiles_ = self.clean_tiles_[shuffler]\n",
    "            self.noise_tiles_ = self.noise_tiles_[shuffler]\n",
    "            \n",
    "            self.dataset_shuffled = True\n",
    "        else:\n",
    "            print(\"Nothing to shuffle, yet. Run 'self.make_dataset()' to make one.\")\n",
    "\n",
    "\n",
    "    def shuffle_rgb_dataset(self):\n",
    "        if hasattr(self, \"clean_tiles_r_\"):\n",
    "            shuffler = np.random.permutation(len(self.clean_tiles_r_))\n",
    "            self.clean_tiles_r_ = self.clean_tiles_r_[shuffler]\n",
    "            self.clean_tiles_g_ = self.clean_tiles_g_[shuffler]\n",
    "            self.clean_tiles_b_ = self.clean_tiles_b_[shuffler]\n",
    "            self.noise_tiles_r_ = self.noise_tiles_r_[shuffler]\n",
    "            self.noise_tiles_g_ = self.noise_tiles_g_[shuffler]\n",
    "            self.noise_tiles_b_ = self.noise_tiles_b_[shuffler]\n",
    "            \n",
    "            self.rgb_dataset_shuffled = True\n",
    "        else:\n",
    "            print(\"Nothing to shuffle, yet. Run 'self.make_rgb_dataset()' to make one.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser():\n",
    "    \"\"\" Works on PIL <Image> objects.\n",
    "    Denoiser.denoise() returns a denoised image, based on the model and\n",
    "    input parameters.\n",
    "    \n",
    "    params\n",
    "    ======\n",
    "    \n",
    "    image: an open PIL Image object\n",
    "    model: a keras trained model object\n",
    "    \n",
    "    tile_size: <int> lenght, in pixel, of the square being denoised\n",
    "        by the model. Higher values use higher amounts of RAM.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image, model,\n",
    "                 tile_size, # <int> or (horizontal pixels, vertical pixels)\n",
    "                 debug=False, verbose=True\n",
    "    ):\n",
    "        self.image = image\n",
    "        self.model = model\n",
    "        \n",
    "        if isinstance(tile_size, tuple):\n",
    "            assert len(tile_size) == 2\n",
    "            self.tile_size_h, self.tile_size_v = tile_size\n",
    "            assert isinstance(self.tile_size_h, (int, float))\n",
    "            assert isinstance(self.tile_size_v, (int, float))\n",
    "        elif isinstance(tile_size, (int, float)):\n",
    "            self.tile_size_h = tile_size\n",
    "            self.tile_size_v = tile_size\n",
    "        else:\n",
    "            raise TypeError(\"tile_size expected as <int> or 2-elems <tuple>\")\n",
    "        \n",
    "        # in case floats were passed\n",
    "        self.tile_size_h = int(self.tile_size_h)\n",
    "        self.tile_size_v = int(self.tile_size_v)\n",
    "        \n",
    "        assert isinstance(debug, bool)\n",
    "        assert isinstance(verbose, bool)\n",
    "        \n",
    "        self.debug = debug\n",
    "        self.verbose = verbose\n",
    "        \n",
    "\n",
    "    def _deb(self, *args, **kwargs):\n",
    "        if self.debug:\n",
    "            print(*args,**kwargs)\n",
    "\n",
    "            \n",
    "    def _say(self, *args, **kwargs):\n",
    "        if self.verbose:\n",
    "            print(*args, **kwargs)\n",
    "\n",
    "\n",
    "    # from Dataset class, *adapted* to be asymmetric\n",
    "    def _crop_in_tiles(self, image, shift=0, asarray=True):\n",
    "\n",
    "        \"\"\"\n",
    "        This generator function crops an image in several tiles\n",
    "        tile_size × tile_size squares, yielding a tile\n",
    "        every iteration.\n",
    "\n",
    "        If the input image is not a perfect multiple of\n",
    "        a(tile_size) × b(tile_size), non-square tiles are NOT\n",
    "        YIELDED.\n",
    "\n",
    "        params\n",
    "        ======\n",
    "\n",
    "        image: a Pillow open image\n",
    "        tile_size: <int> pixels; size of the tile side\n",
    "        shift: <int>: the offset from 0,0 in pixels\n",
    "        \n",
    "        yields\n",
    "        ======\n",
    "        \n",
    "        A tile_size × tile_size np.array taken form the input image,\n",
    "        but converted to float32 type and with values normalized from\n",
    "        0 to 1\n",
    "        \"\"\"\n",
    "\n",
    "        assert isinstance(shift, int)\n",
    "        \n",
    "        width, height = image.size\n",
    "\n",
    "        #calculate coordinates of every tile\n",
    "        for x in range (0 + shift, width, self.tile_size_h):\n",
    "            if width - x < self.tile_size_h:\n",
    "                continue\n",
    "\n",
    "            for y in range (0 + shift, height, self.tile_size_v):\n",
    "                if height - y < self.tile_size_v:\n",
    "                    continue\n",
    "\n",
    "                # tile coord ===\n",
    "                tile_coord = (\n",
    "                    x, y, # upper left coords\n",
    "                    x + self.tile_size_h, y + self.tile_size_v # lower right coords\n",
    "                )\n",
    "\n",
    "                tile = image.crop(tile_coord)\n",
    "\n",
    "                if not asarray:\n",
    "                    yield tile #yielding tile as image\n",
    "                else:\n",
    "                    yield np.array(tile).astype(\"float32\") / 255\n",
    "        \n",
    "        \n",
    "    def _predict_tiles_from_image(self, image, model):\n",
    "        \"\"\" This gives back the denoised <tiles>, according to the loaded <model>\n",
    "        The model operates on multiple tiles at once. All tiles are shaped into a form\n",
    "        that the model was trained for, then all put into a np.array container.\n",
    "        This is the way the models expects the tiles for the prediction.\n",
    "\n",
    "        NOTE: This function relies on crop_in_tiles() function.\n",
    "\n",
    "        params\n",
    "        ======\n",
    "\n",
    "        image: a pillow Image object\n",
    "        model: a keras trained model\n",
    "        \n",
    "        tile_size: <int> pixels. The model will operate and predict on a square with\n",
    "            <tile_size> side. Higher values allocate higher amounts of RAM.\n",
    "            \n",
    "        returns\n",
    "        ======\n",
    "        \n",
    "        A np.array containing all denoised (predicted) tiles\n",
    "        \"\"\"\n",
    "                \n",
    "        to_predict = [\n",
    "            x.reshape(self.tile_size_v, self.tile_size_h, 1) for x in self._crop_in_tiles(image)\n",
    "        ]\n",
    "\n",
    "        # the model expects an array of tiles, not a list of tiles\n",
    "        to_predict = np.array(to_predict)\n",
    "\n",
    "        return model.predict(to_predict)\n",
    "\n",
    "    \n",
    "    def _image_rebuilder(self, image, model):\n",
    "        \"\"\" Takes as input a monochromatic (single-channel) image,\n",
    "        returns a denoised monochromatic image.\n",
    "        \n",
    "        params\n",
    "        ======\n",
    "        \n",
    "        image: a PIL monochromatic image. ONLY ONE channel is supported\n",
    "        model: a trained keras model to denoise the input image\n",
    "        \n",
    "        tile_size: <int> pixels of a square side to process at once.\n",
    "            This is not related to the tile_size the model has been built\n",
    "            with, but dictates how big is the square the model is fed with\n",
    "            for denoising. The bigger this parameter, the more RAM is needed\n",
    "            to perform the denoising.\n",
    "            This cannot be higher than the image phisical size.\n",
    "            \n",
    "        returns\n",
    "        =======\n",
    "        A monochromatic, denoised PIL.Image object\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # I was initially wondering to manage the channel splitting here,\n",
    "        # but as the model is currently working on monochromatic images,\n",
    "        # and will eventually manage the three channels with three different\n",
    "        # models (again, with one channel per image), this stub of implementation\n",
    "        # is not necessary anymore.\n",
    "        # TODO: clear the clutter\n",
    "        channels = [image]\n",
    "\n",
    "        width, height = channels[0].size #all three channels have the same size\n",
    "        self._say(f\"width: {width}; height: {height}\")\n",
    "\n",
    "        # TODO\n",
    "        # for now, we support only exact multiples of tile_size\n",
    "        tile_width = int(width / self.tile_size_h)\n",
    "        tile_height = int(height / self.tile_size_v)\n",
    "\n",
    "        self._say(f\"Image multiple of {tile_width}×{tile_height} integer tiles.\")\n",
    "\n",
    "        for i, channel in enumerate(channels):\n",
    "            \n",
    "            # next line useless if we just process one channel\n",
    "            #self._say(f\"Processing channel {i + 1} of {len(channels)}\")            \n",
    "            pred_tiles = [self._predict_tiles_from_image(channel, model)]\n",
    "\n",
    "            self._deb(f\"Predicted tiles length: {len(pred_tiles[0])}\")\n",
    "\n",
    "            # now we need to rebuild a numpy array based on the tile_width*tile_height original geometry        \n",
    "            gen = (x for x in pred_tiles[0])\n",
    "\n",
    "            # the final assembly is very fast ===\n",
    "            returnimage = []\n",
    "\n",
    "            #for i in range(tile_height):\n",
    "            #    row_tiles = next(gen)\n",
    "            #    for j in range(tile_width - 1):\n",
    "            #        next_tile = next(gen)\n",
    "            #        row_tiles = np.concatenate((row_tiles, next_tile), axis=1)\n",
    "            #    returnimage.append(row_tiles)\n",
    "            #\n",
    "            #returnimage = np.array(returnimage)\n",
    "            #returnimage = np.vstack(returnimage)\n",
    "\n",
    "            for i in range(tile_width):\n",
    "                row_tiles = next(gen)\n",
    "                for j in range(tile_height - 1):\n",
    "                    next_tile = next(gen)\n",
    "                    row_tiles = np.concatenate((row_tiles, next_tile), axis=0)\n",
    "                returnimage.append(row_tiles)\n",
    "\n",
    "            returnimage = np.array(returnimage)\n",
    "            returnimage = np.hstack(returnimage)\n",
    "\n",
    "            # from array (0-1) to Image (0-255)\n",
    "            returnimage = np.uint8(returnimage * 255)\n",
    "            \n",
    "            # discarding the last dimension\n",
    "            return Image.fromarray(returnimage[:,:,0])   \n",
    "\n",
    "        \n",
    "    def denoise(self):\n",
    "        \n",
    "        self._say(\"Denoising red channel..\")\n",
    "        denoised_r = self._image_rebuilder(\n",
    "            self.image.getchannel(\"R\"), self.model\n",
    "        )\n",
    "        \n",
    "        self._say(\"Denoising green channel..\")\n",
    "        denoised_g = self._image_rebuilder(\n",
    "            self.image.getchannel(\"G\"), self.model\n",
    "        )\n",
    "        \n",
    "        self._say(\"Denoising blue channel..\")\n",
    "        denoised_b = self._image_rebuilder(\n",
    "            self.image.getchannel(\"B\"), self.model\n",
    "        )\n",
    "        \n",
    "        rgb = Image.merge(\"RGB\",(denoised_r, denoised_g, denoised_b))\n",
    "        \n",
    "        \n",
    "        self.denoised_ = rgb\n",
    "        del denoised_r, denoised_g, denoised_b\n",
    "        self._say(\"Denoised image in 'denoised_' attribute.\")\n",
    "        \n",
    "        return rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "ds = Dataset(\"standard_dataset\", tile_size=56)\n",
    "ds.make_rgb_dataset()\n",
    "print(ds.rgb_dataset_shuffled)\n",
    "ds.shuffle_rgb_dataset()\n",
    "print(ds.rgb_dataset_shuffled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "source:\n",
    "https://www.machinecurve.com/index.php/2019/12/20/building-an-image-denoiser-with-a-keras-autoencoder-neural-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "-YvMBuyDeIqV"
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "img_width, img_height = 56, 56\n",
    "batch_size = 150\n",
    "no_epochs = 10 # 50\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "max_norm_value = 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_array(array\n",
    "              # img_width, img_height\n",
    "              ):\n",
    "    \n",
    "    global img_width, img_height\n",
    "    \"\"\"Preps an input array for the keras model. Adapted from source:\n",
    "\n",
    "    Reshape data based on channels first / channels last strategy.\n",
    "    This is dependent on whether you use TF, Theano or CNTK as backend.\n",
    "    Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "    \"\"\"\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        array = array.reshape(array.shape[0], 1, img_width, img_height)\n",
    "    else: #\"channels_last\"\n",
    "        array = array.reshape(array.shape[0], img_width, img_height, 1)\n",
    "\n",
    "    array = array.astype('float32')\n",
    "    return array / 255 # Normalize data (0-255 to 0-1)\n",
    "\n",
    "\n",
    "def get_input_shape():\n",
    "\n",
    "    # Adapted from source:\n",
    "    \n",
    "    # Reshape data based on channels first / channels last strategy.\n",
    "    # This is dependent on whether you use TF, Theano or CNTK as backend.\n",
    "    # Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        return (1, img_width, img_height)\n",
    "    else: #\"channels_last\"\n",
    "        return (img_width, img_height, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "ZpNPZp_KOLg_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tiles: 89792\n"
     ]
    }
   ],
   "source": [
    "# input dataset\n",
    "#==============\n",
    "input_noise = prep_array(ds.noise_tiles_r_)\n",
    "input_clean = prep_array(ds.clean_tiles_r_)\n",
    "\n",
    "print(f\"Total tiles: {len(input_noise)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to make <input_train> and <input_test> sets, I will make\n",
    "# them out of input_train, like 80%, 20%\n",
    "#n_elems = input_clean.shape[0]\n",
    "#n_train = int(n_elems * .8)\n",
    "#print(f\"Training on {n_train} / {n_elems} tiles.\")\n",
    "\n",
    "#input_train = input_clean[:n_train].copy()\n",
    "#input_test  = input_clean[n_train:].copy()\n",
    "\n",
    "#noisy_input = input_noise[:n_train].copy()\n",
    "#noisy_input_test = input_noise[n_train:].copy()\n",
    "\n",
    "# this would be passed to validation_data param in keras model and WILL OVERRIDE\n",
    "# validation_split.\n",
    "# if using validation_split, PRE-SHUFFLE the vectors!\n",
    "# see the doc\n",
    "\n",
    "\n",
    "# output arrays for training\n",
    "# =========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjT6aALeRsDD",
    "outputId": "9e84ec2e-45a2-4452-886e-5dde8d38c2fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 54, 54, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 52, 52, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 54, 54, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 56, 56, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 56, 56, 1)         577       \n",
      "=================================================================\n",
      "Total params: 47,425\n",
      "Trainable params: 47,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the model # unchanged from the tutorial, even if we now have 56*56 px tiles\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D\n",
    "          (64,\n",
    "           kernel_size=(3, 3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform',\n",
    "           input_shape=get_input_shape())\n",
    "         )\n",
    "\n",
    "model.add(Conv2D\n",
    "          (32,\n",
    "           kernel_size=(3, 3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model.add(Conv2DTranspose\n",
    "          (32,\n",
    "           kernel_size=(3,3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model.add(Conv2DTranspose\n",
    "          (64,\n",
    "           kernel_size=(3,3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model.add(Conv2D\n",
    "          (1,\n",
    "           kernel_size=(3, 3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='sigmoid',\n",
    "           padding='same')\n",
    "         )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSsc1kMoR5Zi",
    "outputId": "15b1765d-a2fb-47c8-d1c7-393a4664f505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1150/1150 [==============================] - 2078s 2s/step - loss: 0.3484 - val_loss: 0.3393\n",
      "Epoch 2/15\n",
      "1150/1150 [==============================] - 2137s 2s/step - loss: 0.3389 - val_loss: 0.3379\n",
      "Epoch 3/15\n",
      "1150/1150 [==============================] - 2138s 2s/step - loss: 0.3386 - val_loss: 0.3378\n",
      "Epoch 4/15\n",
      "1150/1150 [==============================] - 2131s 2s/step - loss: 0.3386 - val_loss: 0.3385\n",
      "Epoch 5/15\n",
      "1150/1150 [==============================] - 2119s 2s/step - loss: 0.3384 - val_loss: 0.3378\n",
      "Epoch 6/15\n",
      "1150/1150 [==============================] - 2149s 2s/step - loss: 0.3382 - val_loss: 0.3378\n",
      "Epoch 7/15\n",
      "1150/1150 [==============================] - 2116s 2s/step - loss: 0.3381 - val_loss: 0.3377\n",
      "Epoch 8/15\n",
      "1150/1150 [==============================] - 2131s 2s/step - loss: 0.3380 - val_loss: 0.3378\n",
      "Epoch 9/15\n",
      "1150/1150 [==============================] - 2125s 2s/step - loss: 0.3380 - val_loss: 0.3375\n",
      "Epoch 10/15\n",
      "1150/1150 [==============================] - 2119s 2s/step - loss: 0.3380 - val_loss: 0.3376\n",
      "Epoch 11/15\n",
      "1150/1150 [==============================] - 2119s 2s/step - loss: 0.3379 - val_loss: 0.3375\n",
      "Epoch 12/15\n",
      "1150/1150 [==============================] - 2115s 2s/step - loss: 0.3380 - val_loss: 0.3377\n",
      "Epoch 13/15\n",
      "1150/1150 [==============================] - 2114s 2s/step - loss: 0.3379 - val_loss: 0.3375\n",
      "Epoch 14/15\n",
      "1150/1150 [==============================] - 2121s 2s/step - loss: 0.3379 - val_loss: 0.3375\n",
      "Epoch 15/15\n",
      "1150/1150 [==============================] - 2120s 2s/step - loss: 0.3378 - val_loss: 0.3376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e8653a5970>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compile and fit data\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model.fit(input_noise, input_clean,\n",
    "          epochs=no_epochs, #number of epochs\n",
    "          batch_size=batch_size,\n",
    "          validation_split=validation_split,\n",
    "          shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SQ98rgV4R8tx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_ds2_56px_neuralnet_vanilla\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model_ds2_56px_neuralnet_vanilla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "bJDIB5UoCTBG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is DATA\n",
      " Volume Serial Number is A443-7566\n",
      "\n",
      " Directory of D:\\Onedrive\\Python_course\\image denoising\n",
      "\n",
      "07/11/2021  08:57    <DIR>          .\n",
      "07/11/2021  08:57    <DIR>          ..\n",
      "05/11/2021  17:26    <DIR>          .ipynb_checkpoints\n",
      "22/10/2021  10:39            37,065 0.jpg\n",
      "22/10/2021  10:39            40,274 1.jpg\n",
      "22/10/2021  19:15    <DIR>          15_epochs\n",
      "22/10/2021  10:39            35,515 2.jpg\n",
      "22/10/2021  10:39            39,198 3.jpg\n",
      "22/10/2021  10:39            37,044 4.jpg\n",
      "22/10/2021  10:39            35,520 5.jpg\n",
      "21/10/2021  10:05         2,027,330 Building an Image Denoiser with a Keras autoencoder neural network.pdf\n",
      "06/11/2021  19:45            14,724 dataset shooting info.xlsx\n",
      "29/10/2021  17:40           419,734 Denoiser.ipynb\n",
      "01/11/2021  11:06            26,424 Denoiser_with_own_dataset-LAPTOP-BD7NICPD.ipynb\n",
      "07/11/2021  08:56            24,817 Denoiser_with_own_dataset.ipynb\n",
      "06/11/2021  11:39    <DIR>          lino\n",
      "06/11/2021  20:24    <DIR>          model_128px_neuralnet_128_64_64_128_kernel3x3\n",
      "29/10/2021  15:36    <DIR>          model_56px_neuralnet_128_64_64_128_kernel3x3\n",
      "02/11/2021  10:35    <DIR>          model_56px_neuralnet_128_64_64_128_kernel6x6\n",
      "28/10/2021  23:43    <DIR>          model_56px_neuralnet_vanilla\n",
      "07/11/2021  08:57    <DIR>          model_ds2_56px_neuralnet_vanilla\n",
      "07/11/2021  08:57    <DIR>          model_dset2_56px_neuralnet_vanilla\n",
      "06/11/2021  14:44            17,931 Rebuilder.ipynb\n",
      "07/11/2021  00:03    <DIR>          standard_dataset\n",
      "02/11/2021  15:59           (1,201) test 168ž112px.png\n",
      "29/10/2021  13:24    <DIR>          test_tiles\n",
      "              13 File(s)      2,756,777 bytes\n",
      "              13 Dir(s)  37,240,246,272 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 54, 54, 128)       1280      \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 52, 52, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 54, 54, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 56, 56, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 1)         1153      \n",
      "=================================================================\n",
      "Total params: 187,009\n",
      "Trainable params: 187,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try: #I know this is useless.\n",
    "    del model_doubled\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model_doubled = Sequential()\n",
    "multiplier = 2\n",
    "kernel_size=(3, 3)\n",
    "\n",
    "model_doubled.add(Conv2D\n",
    "          (64*multiplier,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform',\n",
    "           input_shape=input_shape)\n",
    "         )\n",
    "\n",
    "model_doubled.add(Conv2D\n",
    "          (32*multiplier,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "\"\"\"\n",
    "model_doubled.add(Conv2D\n",
    "          (32,\n",
    "           kernel_size=(3, 3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_doubled.add(Conv2DTranspose\n",
    "          (32,\n",
    "           kernel_size=(3,3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\"\"\"\n",
    "\n",
    "model_doubled.add(Conv2DTranspose\n",
    "          (32*multiplier,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_doubled.add(Conv2DTranspose\n",
    "          (64*multiplier,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_doubled.add(Conv2D\n",
    "          (1,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='sigmoid',\n",
    "           padding='same')\n",
    "         )\n",
    "\n",
    "model_doubled.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1150/1150 [==============================] - 5142s 4s/step - loss: 0.3484 - val_loss: 0.3390\n",
      "Epoch 2/10\n",
      "1150/1150 [==============================] - 5433s 5s/step - loss: 0.3387 - val_loss: 0.3396\n",
      "Epoch 3/10\n",
      "1150/1150 [==============================] - 5459s 5s/step - loss: 0.3385 - val_loss: 0.3381\n",
      "Epoch 4/10\n",
      "1150/1150 [==============================] - 5435s 5s/step - loss: 0.3382 - val_loss: 0.3379\n",
      "Epoch 5/10\n",
      "1150/1150 [==============================] - 5419s 5s/step - loss: 0.3381 - val_loss: 0.3376\n",
      "Epoch 6/10\n",
      "1150/1150 [==============================] - 5416s 5s/step - loss: 0.3382 - val_loss: 0.3377\n",
      "Epoch 7/10\n",
      "1150/1150 [==============================] - 5400s 5s/step - loss: 0.3380 - val_loss: 0.3376\n",
      "Epoch 8/10\n",
      "1150/1150 [==============================] - 5382s 5s/step - loss: 0.3380 - val_loss: 0.3377\n",
      "Epoch 9/10\n",
      "1150/1150 [==============================] - 5379s 5s/step - loss: 0.3379 - val_loss: 0.3376\n",
      "Epoch 10/10\n",
      "1150/1150 [==============================] - 5390s 5s/step - loss: 0.3379 - val_loss: 0.3374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e82f3f1490>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Compile and fit data #6.5 GB python\n",
    "model_doubled.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model_doubled.fit(noisy_input, pure,\n",
    "                epochs=10, #number of epochs\n",
    "                batch_size=batch_size,\n",
    "                validation_split=validation_split,\n",
    "                shuffle=True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model_ds2_56px_neuralnet_128_64_64_128_kernel3x3\\assets\n"
     ]
    }
   ],
   "source": [
    "model_doubled.save('model_ds2_56px_neuralnet_128_64_64_128_kernel3x3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-shuffle arrays\n",
    "# change kernel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 53, 53, 256)       4352      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 50, 50, 128)       524416    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 48, 48, 64)        73792     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 50, 50, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 53, 53, 128)       131200    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTr (None, 56, 56, 256)       524544    \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 56, 56, 1)         4097      \n",
      "=================================================================\n",
      "Total params: 1,299,329\n",
      "Trainable params: 1,299,329\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3layers = Sequential()\n",
    "outer_layer = 128*2 \n",
    "mid_layer = 64*2\n",
    "inner_layer = 32*2\n",
    "kernel_size=(4, 4)\n",
    "\n",
    "model_3layers.add(Conv2D\n",
    "          (outer_layer,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform',\n",
    "           input_shape=input_shape)\n",
    "         )\n",
    "\n",
    "model_3layers.add(Conv2D\n",
    "          (mid_layer,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_3layers.add(Conv2D\n",
    "          (inner_layer,\n",
    "           kernel_size=(3, 3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_3layers.add(Conv2DTranspose\n",
    "          (inner_layer,\n",
    "           kernel_size=(3,3),\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_3layers.add(Conv2DTranspose\n",
    "          (mid_layer,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_3layers.add(Conv2DTranspose\n",
    "          (outer_layer,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='relu',\n",
    "           kernel_initializer='he_uniform')\n",
    "         )\n",
    "\n",
    "model_3layers.add(Conv2D\n",
    "          (1,\n",
    "           kernel_size=kernel_size,\n",
    "           kernel_constraint=max_norm(max_norm_value),\n",
    "           activation='sigmoid',\n",
    "           padding='same')\n",
    "         )\n",
    "\n",
    "model_3layers.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1150/1150 [==============================] - ETA: 0s - loss: 0.3536 "
     ]
    }
   ],
   "source": [
    "#Compile and fit data #6.5 GB python\n",
    "model_3layers.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "model_3layers.fit(noisy_input, pure,\n",
    "                epochs=5, #number of epochs\n",
    "                batch_size=batch_size,\n",
    "                validation_split=validation_split,\n",
    "                shuffle=True,\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3layers.save('model_ds2_56px_neuralnet_256_128_64_64_128_256_kernel4x4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds48 = Dataset(\"standard_dataset\", tile_size=48)\n",
    "ds48.make_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Denoiser with own dataset",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
